{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_dir = 'C://Users/benjh/Documents/Class/Mod8MovieFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f'{file_dir}/wikipedia-movies.json', mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki_movies_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 records\n",
    "wiki_movies_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 records\n",
    "wiki_movies_raw[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some records in the middle\n",
    "wiki_movies_raw[3600:3605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata = pd.read_csv(f'{file_dir}/movies_metadata.csv', low_memory=False)\n",
    "ratings = pd.read_csv(f'{file_dir}/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kaggle_metadata.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df = pd.DataFrame(wiki_movies_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_movies_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies = [movie for movie in wiki_movies_raw\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie]\n",
    "len(wiki_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies = [movie for movie in wiki_movies_raw\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambada Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda arguments: expression\n",
    "lambda x: x * x\n",
    "square = lambda x: x * x\n",
    "square(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructors \n",
    "##### (Constructors are special functions that can take parameters and initialize a new object using those parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a constructor function\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Alternate movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at movie \"languages\". The first one on the list is Arabic.\n",
    "wiki_movies_df[wiki_movies_df['Arabic'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df[wiki_movies_df['Arabic'].notnull()]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen in previous cell result, there are 193 columns and we cannot make them to b shown here,\n",
    "# bet we can list them by\"\n",
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the Alternate Titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make an empty dict to hold all of the alternative titles (by creating a function).\n",
    "\n",
    "# def clean_movie(movie):\n",
    "#     movie = dict(movie) #create a non-destructive copy\n",
    "#     alt_titles = {}\n",
    "#     return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Loop through a list of all alternative title keys.\n",
    "\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        \n",
    "        return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a: Check if the current key exists in the movie object.\n",
    "\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        if key in movie:\n",
    "            \n",
    "            \n",
    "            return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b: If so, remove the key-value pair and add to the alternative titles dictionary.\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: After looping through every key, add the alternative titles dict to the movie object.\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a list of cleaned movies with a list comprehension:\n",
    "clean_movies = [clean_movie(movie) for movie in wiki_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set wiki_movies_df to be the DataFrame created from clean_movies, and print out a list of the columns.\n",
    "wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Consolidating columns with the same data into one column\n",
    "    # Example:\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use verbs and be explicit in the phrases you make\n",
    "# To change every instance where the key is \"Directed by\" to the new key \"Director,\" \n",
    "# write the following inside clean_movie():change_column_name('Directed by', 'Director'), as an example:\n",
    "\n",
    "# change_column_name('Directed by', 'Director')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid the clean_movie() function look complicated,we should add # some commenting \n",
    "# to make it easier to understand. The whole function should look like this:\n",
    "\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    # combine alternate titles into one list\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune-Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "\n",
    "    # merge column names\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can rerun our list comprehension to clean wiki_movies and recreate wiki_movies_df.\n",
    "\n",
    "clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that the columns are tidied up, time to move on to the rows!\n",
    "# we're going to be using the IMDb ID to merge with the Kaggle data, we want to make sure \n",
    "# that we don't have any duplicate rows.\n",
    "# To extract the ID, we need to use regular expressions or regex method.\n",
    "\n",
    "wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "print(len(wiki_movies_df))\n",
    "wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "print(len(wiki_movies_df))\n",
    "wiki_movies_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that we've consolidated redundant columns (nearly 150 as shown in in above cell) and 41 columns \n",
    "# are remaining), we want to see how many null values are in each column:  \n",
    "\n",
    "[[column,wiki_movies_df[column].isnull().sum()] for column in wiki_movies_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also use a for loop and a print statement.\n",
    "# Either way, we can see about half the columns have more than 6,000 null values. \n",
    "# Let's make a list of columns that have less than 90% null values to trim down our dataset.\n",
    "\n",
    "wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_columns_to_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've reduced 191 messy columns down to 21 useful, data-filled columns as seen in the output.\n",
    "# and th dataframe would look like:\n",
    "wiki_movies_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns also have data stored as text when it should be numeric data or datetimes.\n",
    "# So, we need to identify which columns need to be converted. wiki_movies_df.dtypes will display \n",
    "# the data type for each column.\n",
    "\n",
    "wiki_movies_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking through the data, column by column, we see that:\n",
    "#     Box office should be numeric.\n",
    "#     Budget should be numeric.\n",
    "#     Release date should be a date object.\n",
    "#     Running time should be numeric.\n",
    "# Moreover, the box office and budget amounts aren't written in a consistent style.\n",
    "\n",
    "# So, first we'll make a data series that drops missing values\n",
    "\n",
    "box_office = wiki_movies_df['Box office'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "box_office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions only work on strings, so we'll need to make sure all of the box office data \n",
    "# is entered as a string. By using the apply() method, we can see which values are not strings. \n",
    "# First, make a is_not_a_string() function:\n",
    "\n",
    "def is_not_a_string(x):\n",
    "    return type(x) != str\n",
    "\n",
    "# Then we add \n",
    "box_office[box_office.map(is_not_a_string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of creating a new function with a block of code and the def keyword, \n",
    "# we can create an anonymous lambda function right inside the map() call.\n",
    "# We can update our map() call to use the lambda function directly instead of using is_not_a_string():\n",
    "\n",
    "box_office[box_office.map(lambda x: type(x) != str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a simple space as our joining character and apply the join() function \n",
    "\n",
    "box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (Mod 8.3.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions are just strings of characters that are used as a search pattern. \n",
    "# They are used to test if strings are in a specific format or contain a substring in a \n",
    "# specific format, to extract pertinent information from strings while discarding unnecessary \n",
    "# information, and to perform complicated replacements of substrings.\n",
    "\n",
    "# Regular expressions are used in almost all general-purpose languages like Python. \n",
    "# For example, they are also used in JavaScript, C#, and Java. \n",
    "# Sometimes they are the only viable solution to a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use regular expressions to find out just how many of each style \"\\$123.4 million\" (or billion) \n",
    "# or \"\\$123,456,789.\"are in our data.\"\n",
    "# There is a built-in Python module for regular expressions: re\n",
    "# Python best practices recommend putting all of your import statements at the beginning of your program.\n",
    "\n",
    "import re # the import was added to the top of the codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Box Office Data (Module 8.3.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the First Form to capture this number \"$123.4 million\" (or billion)\n",
    "# the regex\n",
    "form_one = r'\\$\\d+\\.?\\d*\\s*[mb]illion'\n",
    "\n",
    "# the first form\n",
    "box_office.str.contains(form_one, flags=re.IGNORECASE).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the second Form to capture this number, \"$123,456,789.\"\n",
    "# the regex\n",
    "form_two = r'\\$\\d{1,3}(?:,\\d{3})+'\n",
    "box_office.str.contains(form_two, flags=re.IGNORECASE).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to see which values aren't described by either.\n",
    "matches_form_one = box_office.str.contains(form_one, flags=re.IGNORECASE)\n",
    "matches_form_two = box_office.str.contains(form_two, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Values in Forms\n",
    "box_office[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Pattern Matches\n",
    "# 1. Some values have spaces in between the dollar sign and the number.\n",
    "# This is easy to fix. Just add \\s* after the dollar signs.\n",
    "\n",
    "form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illion'\n",
    "\n",
    "form_two = r'\\$\\s*\\d{1,3}(?:,\\d{3})+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Some values use a period as a thousands separator, not a comma.\n",
    "form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're trying to change raw numbers like $123.456.789. We don't want to capture any values \n",
    "# like 1.234 billion so we need the form to look ahead and reject the match if it finds those strings\n",
    "form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Some values are given as a range. \n",
    "# Some editors will convert them in certain situations to em dashes and en dashes. \n",
    "# That is why you are seeing three different types of dashes in the regex expression [-—–] below:\n",
    "box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. \"Million\" is sometimes misspelled as \"millon.\" \n",
    "# we can just make the second \"i\" optional in our match string with a question mark as follows:\n",
    "\n",
    "form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've got expressions to match almost all the box office values, \n",
    "# we'll use them to extract only the parts of the strings that match\n",
    "box_office.str.extract(f'({form_one}|{form_two})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we need a function to turn the extracted values into a numeric value. \n",
    "# We'll call it parse_dollars, and parse_dollars will take in a string and return a floating-point number. \n",
    "def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "    if type(s) != str:\n",
    "        return np.nan\n",
    "\n",
    "    # if input is of the form $###.# million\n",
    "    if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a million\n",
    "        value = float(s) * 10**6\n",
    "\n",
    "        # return value\n",
    "        return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "    elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a billion\n",
    "        value = float(s) * 10**9\n",
    "\n",
    "        # return value\n",
    "        return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "    elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "        s = re.sub('\\$|,','', s)\n",
    "\n",
    "        # convert to float\n",
    "        value = float(s)\n",
    "\n",
    "        # return value\n",
    "        return value\n",
    "\n",
    "    # otherwise, return NaN\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have everything we need to parse the box office values to numeric values.\n",
    "\n",
    "wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "\n",
    "wiki_movies_df['box_office']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we no longer need the Box Office column, so we'll just drop it:\t\n",
    "\n",
    "wiki_movies_df.drop('Box office', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Budget (Module 8.3.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We parsed the box office data and we will use the same pattern matches and see \n",
    "# how many budget values are in a different form.\n",
    "\n",
    "#Create a budget variable with the following code:\n",
    "budget = wiki_movies_df['Budget'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any lists to strings:\n",
    "\n",
    "budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then remove any values between a dollar sign and a hyphen (for budgets given in ranges):\n",
    "\n",
    "budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same pattern matches that we created to parse the box office data, \n",
    "# and apply them without modifications to the budget data.\t\n",
    "matches_form_one = budget.str.contains(form_one, flags=re.IGNORECASE)\n",
    "matches_form_two = budget.str.contains(form_two, flags=re.IGNORECASE)\n",
    "budget[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That parsed almost all of the budget data. However, there's a new issue with the budget data: \n",
    "# citation references (the numbers in square brackets).\n",
    "\n",
    "# We can remove those fairly easily with a regular expression.\n",
    "# \\[\\d+\\] \n",
    "\n",
    "# Remove the citation references with the following:\n",
    "\n",
    "budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "budget[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There will be 30 budgets remaining.\n",
    "# Is it worth our time to try and parse what we can out of these remaining 30 budget values, \n",
    "# or should we just drop them?\n",
    "# A handful of them don't even have numeric values, and those that do tend to be in a different currency.\n",
    "\n",
    "# There are a handful of values that could be parsed into usable data points without \n",
    "# worrying about currency conversion, but we have almost 4,700 other budget values \n",
    "# to work with, so even 30 values is less than 1% of the data.\n",
    "\n",
    "# Everything is now ready to parse the budget values. We can copy the line of code we used to \n",
    "# parse the box office values, changing \"box_office\" to \"budget\":\n",
    "\n",
    "wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df['budget'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now drop the original Budget column.\n",
    "\n",
    "wiki_movies_df.drop('Budget', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the release date will follow a similar pattern to parsing box office and budget, \n",
    "#but with different forms.\n",
    "\n",
    "# First, make a variable that holds the non-null values of Release date in the DataFrame, \n",
    "#converting lists to strings:\n",
    "\n",
    "release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forms we'll be parsing are:\n",
    "\n",
    "# Full month name, one- to two-digit day, four-digit year (i.e., January 1, 2000)\n",
    "# Four-digit year, two-digit month, two-digit day, with any separator (i.e., 2000-01-01)\n",
    "# Full month name, four-digit year (i.e., January 2000)\n",
    "\n",
    "# One way to parse those forms is with the following:\n",
    "date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "date_form_four = r'\\d{4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last four statements in the cell above: \n",
    "    # The first matches the month, dd, yyyy format. \n",
    "    # The second matches yyyy.mm.dd. \n",
    "    # The third matches month yyyy. \n",
    "    # The fourth matches yyyy. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And then we can extract the dates with:\t\n",
    "release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of creating our own function to parse the dates, we'll use the built-in to_datetime() method \n",
    "# in Pandas. Since there are different date formats, set the infer_datetime_format option to True. \n",
    "# The date formats we've targeted are among those that the to_datetime() function can recognize, \n",
    "# which explains the infer_datetime_format=True argument below.\n",
    "\n",
    "wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Running Time\n",
    "# First, make a variable that holds the non-null values of Release date in the DataFrame, \n",
    "# converting lists to strings:\n",
    "\n",
    "running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like most of the entries just look like \"100 minutes.\n",
    "# Let's see how many running times look exactly like that by using \"string\" boundaries.\n",
    "\n",
    "running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code returns 6,528 entries. Let's get a sense of what the other 366 entries look like.\n",
    "\n",
    "running_time[running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE) != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this more general by only marking the beginning of the string, and accepting \n",
    "# other abbreviations of \"minutes\" by only searching up to the letter \"m.\"\n",
    "\n",
    "running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That accounts for 6,877 entries. The remaining 17 follow:\n",
    "running_time[running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE) != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to extract digits, and we want to allow for both possible patterns. \n",
    "# Therefore, we'll add capture groups around the \\d instances as well as add an alternating character. \n",
    "running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_time_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This new DataFrame is all strings, we'll need to convert them to numeric values. \n",
    "\n",
    "# Because we may have captured empty strings, we'll use the to_numeric() method and \n",
    "# set the errors argument to 'coerce'. Coercing the errors will turn the empty strings \n",
    "# into Not a Number (NaN), then we can use fillna() to change all the NaNs to zeros.\"\n",
    "\n",
    "running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_time_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply a function to convert the hour and minute capture groups to minutes \n",
    "# if the pure minutes capture group is zero, and save the output to wiki_movies_df:\n",
    "\n",
    "wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we drop Running time from the dataset with the following code:\n",
    "\n",
    "wiki_movies_df.drop('Running time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Kaggle Data (Module 8.3.12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kaggle data that Britta found is much more structured, but it still requires some cleaning, \n",
    "# including converting strings to correct data types.\n",
    "\n",
    "# Initial Look at the Movie Metadata to see all of the columns in the csv file are the correct data types\n",
    "kaggle_metadata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data types in Kaggle dataset\n",
    "#     release_date    datetime\n",
    "#     popularity\tnumeric\n",
    "#     ID\tnumeric\n",
    "#     video\tBoolean\n",
    "#     adult\tBoolean\n",
    "#     budget\tnumeric\n",
    "# We'll convert the data types for each of the six columns that need to be converted.\n",
    "\n",
    "# Before we convert the \"adult\" and \"video\" columns, let's check that all the values are either True or False.\n",
    "\n",
    "kaggle_metadata['adult'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some bad data in here. Let's remove it.\t\n",
    "\t\n",
    "# To remove the bad data, use the following:\t\n",
    "\n",
    "kaggle_metadata[~kaggle_metadata['adult'].isin(['True','False'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a closer look at the three movies there appears to be corrupted data:\n",
    "# Somehow the columns got scrambled for these three movies.\n",
    "#We don't want to include adult movies, therefore, we'll only keep rows where \n",
    "# adult is False, and then drop the \"adult\" column.\n",
    "\n",
    "#The following code will keep rows where the adult column is False, and then drop the adult column.\n",
    "kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we'll look at the values of the video column:\t\n",
    "kaggle_metadata['video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are only false and true datatypes that we can easily convert them\n",
    "\n",
    "kaggle_metadata['video'] == 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code creates the Boolean column we want. We just need to assign it back to video:\t\n",
    "kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the numeric columns, we can just use the to_numeric() method from Pandas. \n",
    "# We'll make sure the errors= argument is set to 'raise', so we'll know if there's \n",
    "# any data that can't be converted to numbers.\n",
    "\n",
    "kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we need to convert release_date to datetime. \n",
    "# Pandas has a built-in function for that as well: to_datetime().\t \t \n",
    "# Since release_date is in a standard format, to_datetime() will convert it without any fuss.\t\n",
    "\n",
    "kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are done with cleaning the Kaggle metadata!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasonability Checks on Ratings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, we'll take a look at the ratings data. \n",
    "# We'll use the info() method on the DataFrame. \n",
    "# Since the ratings dataset has so many rows, we need to set the null_counts option to True.\n",
    "\n",
    "ratings.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll specify in to_datetime() that the origin is \"unix\" and the time unit is \"seconds\".\t\n",
    "pd.to_datetime(ratings['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the output looks reasonable, assign it to the timestamp column. \n",
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** To show the background of the histogram (next cell's output) in the current jupyter notebook theme.\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we'll look at the statistics of the ratings and see if there are any errors. \n",
    "# A quick, easy way is to a histogram of the rating distributions, then use the\n",
    "# describe() method to print out some stats on central tendency and spread.\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "ratings['rating'].plot(kind='hist')\n",
    "ratings['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Wikipedia and Kaggle Metadata (Module 8.4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia data and Kaggle data are cleaned up and in tabular formats with the right \n",
    "# data types for each column, we can join them together.\n",
    "# ut it still needs to be cleaned up a bit, especially where both overlap.\n",
    "# We only want movies that are in both tables, an INNER JOIN is an appropriate join to use.\n",
    "# One of the things we always want to look out for after merging data is redundant columns.\n",
    "# Let's print out a list of the columns to identify redundant columns. \n",
    "# We'll use the suffixes parameter to make it easier to identify which table each column came from. \n",
    "\n",
    "movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are seven pairs of columns that have redundant information.\n",
    "# We'll look at each pair of columns and decide how to handle the data.\n",
    "# Below is the list of competing columns. We'll fill in the resolution to each pair.\n",
    "    \n",
    "# Competing data:\n",
    "\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia. [see COMMENT 1 below.]\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data\n",
    "# box_office               revenue                  Keep Kaggle; fill in zeros with Wikipedia data\n",
    "# release_date_wiki        release_date_kaggle      Keep Kaggle; drop Wikipedia release dates\n",
    "# Language                 original_language        Keep Kaggle; drop Wikipedia.\n",
    "# Production company(s)    production_companies     Keep Kaggle; drop Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  <<< 1- Title >>>\n",
    "# Take a quick look at some of the titles.\n",
    "\n",
    "movies_df[['title_wiki','title_kaggle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They both seem consistent, which we'd expect.\n",
    "# Look at the rows where the titles don't match.\t\n",
    "movies_df[movies_df['title_wiki'] != movies_df['title_kaggle']][['title_wiki','title_kaggle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both options look pretty good, but the Kaggle data looks just a little more consistent.\n",
    "\n",
    "# Show any rows where title_kaggle is empty\n",
    "movies_df[(movies_df['title_kaggle'] == '') | (movies_df['title_kaggle'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT !:  No results were returned, so we can just drop the Wikipedia titles. \n",
    "# Next, look at running_time versus runtime. \n",
    "# A scatter plot is a great way to give us a sense of how similar the columns are to each other. \n",
    "# Scatter plot would not show null or missing values\n",
    "# The following code will fill in missing values with \"zero\" and make the scatter plot:\n",
    "\n",
    "movies_df.fillna(0).plot(x='running_time', y='runtime', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are more data points on the origin of the Y axis that is Kaggle, this means there are more missing\n",
    "# entries in the Wikipedia data set \n",
    "# Also, most of the runtimes are pretty close to each other but the Wikipedia data has some outliers\n",
    "# We can also see from the scatter plot that there are movies where Kaggle has 0 for the runtime but\n",
    "# Wikipedia has data, so we'll fill in the gaps with Wikipedia data. \n",
    "# ---> drop wiki but fill the missing data from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<< 2- Budget >>>>\n",
    "# Since budget_wiki and budget_kaggle are numeric, we'll make another scatter plot to compare the values:\n",
    "\n",
    "movies_df.fillna(0).plot(x='budget_wiki',y='budget_kaggle', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Wikipedia data appears to have more outliers \n",
    "# However, there are quite a few movies with no data in the Kaggle column, \n",
    "# Therefore, we'll fill in the gaps with Wikipedia's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<< 3- Box Office >>>>\n",
    "# The box_office and revenue columns are numeric, so we'll make another scatter plot.\n",
    "\n",
    "movies_df.fillna(0).plot(x='box_office', y='revenue', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots looks pretty close, but we might be getting thrown off by the scale of large\n",
    "# data point. Let's look at the scatter plot for everything less than $1 billion in box_office.\n",
    "\n",
    "movies_df.fillna(0)[movies_df['box_office'] < 10**9].plot(x='box_office', y='revenue', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks similar to what we've seen for budget, so keep the Kaggle data, but fill in \n",
    "# the zeros with Wikipedia data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<< 4- Release Date >>>>\n",
    "# For release dates, we can't directly make a scatter plot, as it only works on numeric data. \n",
    "# However, there's a tricky workaround. We'll use the regular line plot (which can plot date data), \n",
    "# and change the style to only put dots by adding style='.' to the plot() method:\n",
    "\n",
    "\n",
    "# movies_df[['Release date','release_date']].plot(x='Release date', y='release_date', style='.')\n",
    "\n",
    "movies_df[['release_date_wiki','release_date_kaggle']].plot(x='release_date_wiki', y='release_date_kaggle', style='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should investigate that wild outlier around 2006. \n",
    "# We're just going to choose some rough cutoff dates to single out that one movie. \n",
    "# We'll look for any movie whose release date according to Wikipedia is after 1996, but whose \n",
    "# release date according to Kaggle is before 1965. \n",
    "\n",
    "movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output, it looks like somehow The Holiday in the Wikipedia data got merged \n",
    "# with From Here to Eternity. We'll have to drop that row from our DataFrame. \n",
    "# We'll get the index of that row with the following:\n",
    "\n",
    "movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can drop that row like this:\n",
    "\n",
    "movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, see if there are any null values:\n",
    "\n",
    "movies_df[movies_df['release_date_wiki'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Wikipedia data is missing release dates for 11 movies:\n",
    "# But the Kaggle data isn't missing any release dates. In this case, we'll just drop the Wikipedia data.\n",
    "\n",
    "#If we take a look at what columns we have, we will see that release_date_wiki is there and needs to go \n",
    "movies_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should drop release_date_wiki here.\n",
    "\n",
    "# ...........drop('release_date_wiki',axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<< 5- Language >>>>\n",
    "\n",
    "# For the language data, we'll compare the value counts of each. However, consider the following code:\n",
    "\n",
    "movies_df['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code above throws an error because some of the language data points are stored as lists.\n",
    "# TypeError: unhashable type: 'list'\n",
    "\n",
    "# We need to convert the lists in Language to tuples so that the value_counts() method will work. \n",
    "\n",
    "movies_df['Language'].apply(lambda x: tuple(x) if type(x) == list else x).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Kaggle data, there are no lists, so we can just run value_counts() on it.\n",
    "\n",
    "movies_df['original_language'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<< 6- Production Companies >>>>\n",
    "\n",
    "# We'll start off just taking a look at a small number of samples.\n",
    "\n",
    "movies_df[['Production company(s)','production_companies']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kaggle data is much more consistent, and it would be difficult, if not impossible, to translate the Wikipedia data into the same format.\n",
    "# We'll drop the Wikipedia data in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Put It All Together\n",
    "# First, we'll drop the title_wiki, release_date_wiki, Language, and Production company(s) columns.\n",
    "\n",
    "movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, to save a little time, we'll make a function that fills in missing data for a \n",
    "# column pair and then drops the redundant column.\n",
    "\n",
    "def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "    df[kaggle_column] = df.apply(\n",
    "        lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "        , axis=1)\n",
    "    df.drop(columns=wiki_column, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run the function for the three column pairs that we decided to fill in zeros.\n",
    "\n",
    "fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we've merged our data and filled in values, it's good to check that there aren't any \n",
    "# columns with only one value, since that doesn't really provide any information. \n",
    "# We also need to convert lists to tuples for value_counts() to work.\n",
    "\n",
    "for col in movies_df.columns:\n",
    "    lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "    value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "    num_values = len(value_counts)\n",
    "    if num_values == 1:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this, we see that 'video' only has one value:\n",
    "\n",
    "movies_df['video'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should reorder the columns to make the dataset easier to read for the hackathon participants. \n",
    "# Having similar columns near each other helps people looking through the data get a better sense \n",
    "# of what information is available.\n",
    "\n",
    "movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we need to rename the columns to be consistent.\n",
    "\n",
    "movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first part of merge between Wikipeda and Kaggle data is complete.\n",
    "# We will push this to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming and Merging Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to include the rating data with the movie data, but it's a very large dataset. \n",
    "# We need to reduce the ratings data to a useful summary of rating information for each movie, \n",
    "# and then make the full dataset available to the hackathon participants, if needed.\n",
    "\n",
    " # First, we need to use a groupby on the \"movieId\" and \"rating\" columns and take the count for each group. \n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we'll rename the \"userId\" column to \"count.\"\n",
    "# The choice of renaming \"userId\" to \"count\" is arbitrary. Both have the same information, \n",
    "# and we could use either one.\n",
    "\n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the magical part. We can pivot this data so that movieId is the index, the columns \n",
    "# will be all the rating values, and the rows will be the counts for each rating value.\n",
    "\n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to rename the columns so they're easier to understand. \n",
    "# We'll prepend rating_ to each column with a list comprehension:\n",
    "\n",
    "rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we need to use a left merge, since we want to keep everything in movies_df:\n",
    "\n",
    "movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, because not every movie got a rating for each rating level, there will be \n",
    "# missing values instead of zeros. We have to fill those in ourselves, like this:\n",
    "    \n",
    "movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_with_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we finished the Transform step in ETL! \n",
    "# Next we will loadour tables into SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Pandas and SQL (Module 8.5.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now that we've extracted and transformed our data, it's time to load it into a SQL database. \n",
    "# We're going to create a new database and use the built-in to_sql() method in Pandas to create \n",
    "# a table for our merged movie data.\n",
    "\n",
    "# We need to import create_engine from the sqlalchemy module. (add this import to the first cell).\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import db_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our local server, the connection string will be as follows:\n",
    "# This is all the information that SQLAlchemy needs to create a database engine.\n",
    "db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine with the following:\n",
    "\n",
    "# But some users may need an additional package installed before running the folowing code. \n",
    "# In [Anaconda] terminal, run the following code: pip install psycopg2-binary \n",
    "# to add it to your coding environment.\n",
    "\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the movies_df DataFrame to a SQL table, we only have to specify the name of the table \n",
    "# and the engine in the to_sql() method.\n",
    "\n",
    "movies_df.to_sql(name='movies', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pgAdmin, confirm that the table imported correctly. Follow these steps:\n",
    "\n",
    "    # Right-click the \"movies\" table name and select Properties.\n",
    "    # Click the Columns tab to make sure all columns have an appropriate data type.\n",
    "    # Close the Properties window, and then right-click \"movies\" again.\n",
    "    # Select \"View/Edit Data\" followed by \"First 100 Rows.\"\n",
    "    # Right-click \"movies\" and select Query Tool.\n",
    "    # Inside the Query Editor, run the query \"select count(*) from movies\" to make sure all the rows \n",
    "    # were imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's time to import the ratings data.\n",
    "# The ratings data is too large to import in one statement, so it has to be divided into \"chunks\" of data. \n",
    "# To do so, we'll need to reimport the CSV using the chunksize= parameter in read_csv(). \n",
    "# This creates an iterable object, so we can make a for loop and append the chunks of data to the new \n",
    "# rows to the target SQL table.\n",
    "# This can take quite a long time to run (more than an hour). \n",
    "# It's a really good idea to print out some information about how it's running.\n",
    "# We will add functionality for the print out:\n",
    "    # How many rows have been imported\n",
    "    # How much time has elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <<<<< Step 1: Print Number of Imported Rows >>>>>\n",
    "    # create a variable for the number of rows imported\n",
    "\n",
    "# <<<<< Step 2: Print Elapsed Time >>>>>\n",
    "    # We'll use the built-in time module in Python. \n",
    "    # time.time() returns the current time whenever it is called. \n",
    "    # Subtracting two time values gives the difference in seconds. \n",
    "    # By setting a variable at the beginning to the time at the start, inside the loop \n",
    "    # we can easily calculate elapsed time and print it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: (to prevent the code below throw an error I typed a forward slash before ratings.vcsv)\n",
    "# We're going to print the total amount of time elapsed at every step. \n",
    "\n",
    "import time\n",
    "\n",
    "rows_imported = 0\n",
    "# get the start_time from time.time()\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    # add elapsed time to final print out\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the cell finishes running, confirm the table imported correctly using pgAdmin. \n",
    "\n",
    "# To do so: Right click on the movies table >> View/Edit Data >> First 100 rows\n",
    "\n",
    "# Verify the columns have the correct data type, inspect the first 100 rows, and check the row count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then make sure you have added not to track code to ignore file on GitUb\n",
    "        # Open the .gitignore file, and on the first line type the following:\n",
    "\n",
    "        # Adding config.py file.\n",
    "        # config.py\n",
    "        \n",
    "# And finally send the files to GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
